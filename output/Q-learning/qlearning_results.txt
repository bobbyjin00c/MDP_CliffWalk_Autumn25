Q-learning算法实验结果
==================================================

环境参数:
网格大小: 4 x 12
起点位置: (3, 0)
终点位置: (3, 11)
悬崖位置: [(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10)]

训练参数:
训练回合数: 1000
学习率(alpha): 0.1
折扣因子(gamma): 1.0
探索率(epsilon): 0.1

性能统计:
平均奖励(最后100回合): -55.28
奖励标准差: 73.54
最佳奖励: -12
最差奖励: -342
平均步数: 17.7

最优策略矩阵:
0 2 1 0 1 1 2 2 0 2 1 2
1 1 1 2 2 2 2 1 1 1 1 2
1 1 1 1 1 1 1 1 1 1 1 2
0 0 0 0 0 0 0 0 0 0 0 0

(0=上, 1=右, 2=下, 3=左)

Q值统计:
最大Q值: 0.0000
最小Q值: -111.6643
平均Q值: -10.9191

算法特点说明:
Q-learning是off-policy算法，在线性能可能较差
但能收敛到理论最优策略
