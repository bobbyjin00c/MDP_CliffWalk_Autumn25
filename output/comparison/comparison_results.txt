SARSA vs Q-learning 算法比较结果
============================================================

实验设置:
训练回合数: 1000
学习率(alpha): 0.1
折扣因子(gamma): 1.0
探索率(epsilon): 0.1

性能比较结果:
----------------------------------------

SARSA算法:
  平均奖励(最后100回合): -25.59
  奖励标准差: 29.16
  平均步数: 17.7
  掉崖次数/100次测试: 0
  路径安全性: 100%
  收敛回合数: 362

Q-learning算法:
  平均奖励(最后100回合): -42.43
  奖励标准差: 57.91
  平均步数: 16.7
  掉崖次数/100次测试: 0
  路径安全性: 100%
  收敛回合数: 430

========================================
关键发现:
1. SARSA在线性能更好，表现更稳定
2. SARSA学习到更安全的路径，掉崖次数更少
3. Q-learning可能收敛到理论最优但风险更高的路径
4. 在悬崖行走环境中，SARSA通常表现更好

算法选择建议:
- 高风险环境: 选择SARSA
- 追求理论最优: 选择Q-learning
- 在线学习: 选择SARSA
- 离线学习: 选择Q-learning
